{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Given that we are essentially looking for relationships between words when creating something like a large language model (LLM), we have to prepare the data in such a way that is conducive to modeling purposes. It unfortunately is very inefficient to use the raw words with no alteration. Imagine all the conjugations of any English verb. For example, consider the word \"drop\". When used as a verb, \"drop\" can manifest as any of the following:\n",
    "\n",
    "- drop\n",
    "- dropped\n",
    "- dropping\n",
    "\n",
    "To further complicate the matter, \"drop\" can also be used as a noun (as in a water droplet or rain drop).\n",
    "\n",
    "So how do we get around this? Prior to training an LLM, we convert the input into new representations referred to as **tokens**. Generally speaking, a token represents a 3-5 character word (or part of a longer word); however, tokens can also represent other things including emojis, punctuation, and more.\n",
    "\n",
    "In recent times, many have converged on a singular strategy for tokenization, and that is called **Byte Pair Encoding (BPE)**. At a high level, BPE seeks to chunk words down into byte pairs, and if there's a particular byte pair that is showing up more often than others, we iterate back through the full vocabulary and update anywhere we saw that popular byte pair with an entirely new token. We keep doing this over and over until we're basically happy with the results.\n",
    "\n",
    "*Note: One thing I personally am struggling to understand is what the ideal number of iterations (aka merges) to do. This is something I'll have to come back and update later.*\n",
    "\n",
    "For example, let's say the byte pair reprseenting the letters \"th\" show up quite often. We will then go back through and update each entry in the vocabulary that has \"th\" with another arbitrary value, like the letter \"Z\". (This is just an example. Don't worry too much about precise correctness for now.) As you can imagine, representing a two-character string and a single-character string is more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "Throughout this notebook, we will largely make use of standard Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
