{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Given that we are essentially looking for relationships between words when creating something like a large language model (LLM), we have to prepare the data in such a way that is conducive to modeling purposes. It unfortunately is very inefficient to use the raw words with no alteration. Imagine all the conjugations of any English verb. For example, consider the word \"drop\". When used as a verb, \"drop\" can manifest as any of the following:\n",
    "\n",
    "- drop\n",
    "- dropped\n",
    "- dropping\n",
    "\n",
    "To further complicate the matter, \"drop\" can also be used as a noun (as in a water droplet or rain drop).\n",
    "\n",
    "So how do we get around this? Prior to training an LLM, we convert the input into new representations referred to as **tokens**. Generally speaking, a token represents a 3-5 character word (or part of a longer word); however, tokens can also represent other things including emojis, punctuation, and more.\n",
    "\n",
    "In recent times, many have converged on a singular strategy for tokenization, and that is called **Byte Pair Encoding (BPE)**. At a high level, BPE seeks to chunk words down into byte pairs, and if there's a particular byte pair that is showing up more often than others, we iterate back through the full vocabulary and update anywhere we saw that popular byte pair with an entirely new token. We keep doing this over and over until we're basically happy with the results.\n",
    "\n",
    "*Note: One thing I personally am struggling to understand is what the ideal number of iterations (aka merges) to do. This is something I'll have to come back and update later.*\n",
    "\n",
    "For example, let's say the byte pair reprseenting the letters \"th\" show up quite often. We will then go back through and update each entry in the vocabulary that has \"th\" with another arbitrary value, like the letter \"Z\". (This is just an example. Don't worry too much about precise correctness for now.) As you can imagine, representing a two-character string and a single-character string is more efficient!\n",
    "\n",
    "In a happy coincidence, I began working on this notebook at the same time that AI master [Andrej Karpathy](https://karpathy.ai) also started work to explain this same concept. This notebook will be largely based upon [his work in this GitHub repo](https://github.com/karpathy/minbpe/tree/master)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "Throughout this notebook, we will largely make use of standard Python code. As a simple test, we will be following Andrej's lead by making use of [the same example found on this Wikipedia page](https://en.wikipedia.org/wiki/Byte_pair_encoding). Namely, this example takes the following sample input text:\n",
    "\n",
    "```\n",
    "sample_text = 'aaabdaaabac'\n",
    "```\n",
    "\n",
    "After three \"runs\" (aka merges) of the BPE algorithm, the expected outbook should be:\n",
    "\n",
    "```\n",
    "expected_output = 'XdXac'\n",
    "```\n",
    "\n",
    "The output decomposes into the following sources:\n",
    "\n",
    "```\n",
    "X = ZY\n",
    "Y = ab\n",
    "Z = aa\n",
    "```\n",
    "\n",
    "To ensure that everything is working as expected, we will revisit this sample throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
