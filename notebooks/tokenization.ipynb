{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Given that we are essentially looking for relationships between words when creating something like a large language model (LLM), we have to prepare the data in such a way that is conducive to modeling purposes. It unfortunately is very inefficient to use the raw words with no alteration. Imagine all the conjugations of any English verb. For example, consider the word \"drop\". When used as a verb, \"drop\" can manifest as any of the following:\n",
    "\n",
    "- drop\n",
    "- dropped\n",
    "- dropping\n",
    "\n",
    "To further complicate the matter, \"drop\" can also be used as a noun (as in a water droplet or rain drop).\n",
    "\n",
    "So how do we get around this? Prior to training an LLM, we convert the input into new representations referred to as **tokens**. Generally speaking, a token represents a 3-5 character word (or part of a longer word); however, tokens can also represent other things including emojis, punctuation, and more.\n",
    "\n",
    "In recent times, many have converged on a singular strategy for tokenization, and that is called **Byte Pair Encoding (BPE)**. At a high level, BPE seeks to chunk words down into byte pairs, and if there's a particular byte pair that is showing up more often than others, we iterate back through the full vocabulary and update anywhere we saw that popular byte pair with an entirely new token. We keep doing this over and over until we're basically happy with the results.\n",
    "\n",
    "*Note: One thing I personally am struggling to understand is what the ideal number of iterations (aka merges) to do. This is something I'll have to come back and update later.*\n",
    "\n",
    "For example, let's say the byte pair reprseenting the letters \"th\" show up quite often. We will then go back through and update each entry in the vocabulary that has \"th\" with another arbitrary value, like the letter \"Z\". (This is just an example. Don't worry too much about precise correctness for now.) As you can imagine, representing a two-character string and a single-character string is more efficient!\n",
    "\n",
    "In a happy coincidence, I began working on this notebook at the same time that AI master [Andrej Karpathy](https://karpathy.ai) also started work to explain this same concept. This notebook will be largely based upon [his work in this GitHub repo](https://github.com/karpathy/minbpe/tree/master)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "Throughout this notebook, we will largely make use of standard Python code. As a simple test, we will be following Andrej's lead by making use of [the same example found on this Wikipedia page](https://en.wikipedia.org/wiki/Byte_pair_encoding). Namely, this example takes the following sample input text:\n",
    "\n",
    "```\n",
    "sample_text = 'aaabdaaabac'\n",
    "```\n",
    "\n",
    "After three \"runs\" (aka merges) of the BPE algorithm, the expected outbook should be:\n",
    "\n",
    "```\n",
    "expected_output = 'XdXac'\n",
    "```\n",
    "\n",
    "The output decomposes into the following sources:\n",
    "\n",
    "```\n",
    "X = ZY\n",
    "Y = ab\n",
    "Z = aa\n",
    "```\n",
    "\n",
    "To ensure that everything is working as expected, we will revisit this sample throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the sample text and expected output\n",
    "sample_text = 'aaabdaaabac'\n",
    "expected_output = 'XdXac'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic BPE Tokenizer\n",
    "In this section, we'll start building out the basic BPE tokenizer using standard Python code. This will serve as a basis for sections later on. To keep things simple, we'll take things one step at a time with a robust explanation to ensure you maintain a full intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTF-8 Byte Encoding\n",
    "So far, the examples we have used so far have used strings in their more \"natural\" form, but this is not actually what we're going to use for the final tokenizer. Remember, words are NOT the only things we tokenize! We also tokenize punctuation, emojis, and more.\n",
    "\n",
    "What we're going to do is to first convert our input text into their **raw byte values**. Remember, your computer's processor basically operates on calculating a bunch of ones and zeroes, so when you press the letter \"a\" on your keyboard, your computer is understanding it as something it can work with. The most common standard for managing this is known as the **Unicode Transformation Format - 8-bit**, more commonly known as **UTF-8**. Let's go ahead and demonstrate an example in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is my name (David Hundley) as UTF-8 encoded bytes:\n",
      "[68, 97, 118, 105, 100, 32, 72, 117, 110, 100, 108, 101, 121]\n",
      "\n",
      "Here is what the ðŸ¥³ emoji looks like in byte form:\n",
      "[240, 159, 165, 179]\n"
     ]
    }
   ],
   "source": [
    "# Printing the UTF-8 version of my name\n",
    "name_bytes = list('David Hundley'.encode('utf-8'))\n",
    "print(f'Here is my name (David Hundley) as UTF-8 encoded bytes:\\n{name_bytes}')\n",
    "\n",
    "# Printing the UTF-8 version of an emoji\n",
    "emoji_bytes = list('ðŸ¥³'.encode('utf-8'))\n",
    "print(f'\\nHere is what the ðŸ¥³ emoji looks like in byte form:\\n{emoji_bytes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we encode just characters as we do in my name, we get a single integer byte value for each individual letter in my name. However, when we feed through a single emoji, that emoji is represented by a collection of 4 separate integer byte values.\n",
    "\n",
    "Let's go ahead and apply this same UTF-8 encoding to our sample input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text bytes:\n",
      "[97, 97, 97, 98, 100, 97, 97, 97, 98, 97, 99]\n"
     ]
    }
   ],
   "source": [
    "# Converting the sample to a list of UTF-8 encoded bytes\n",
    "sample_text_bytes = list(sample_text.encode('utf-8'))\n",
    "print(f'Sample text bytes:\\n{sample_text_bytes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
