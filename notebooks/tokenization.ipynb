{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Given that we are essentially looking for relationships between words when creating something like a large language model (LLM), we have to prepare the data in such a way that is conducive to modeling purposes. It unfortunately is very inefficient to use the raw words with no alteration. Imagine all the conjugations of any English verb. For example, consider the word \"drop\". When used as a verb, \"drop\" can manifest as any of the following:\n",
    "\n",
    "- drop\n",
    "- dropped\n",
    "- dropping\n",
    "\n",
    "To further complicate the matter, \"drop\" can also be used as a noun (as in a water droplet or rain drop).\n",
    "\n",
    "So how do we get around this? Prior to training an LLM, we convert the input into new representations referred to as **tokens**. Generally speaking, a token represents a 3-5 character word; however, tokens can also represent other things including emojis, punctuation, and more.\n",
    "\n",
    "In recent times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
